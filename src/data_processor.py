"""
Data Processor - ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ SRF-SQL pairs process ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
Simple ‡¶è‡¶¨‡¶Ç easy to understand
Enhanced with metadata extraction
"""
import json
import os
import pandas as pd
import re
from pathlib import Path
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataProcessor:
    """SRF-SQL data process ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø simple class"""
    
    def __init__(self, data_path="./data/training_data"):
        self.data_path = Path(data_path)
        self.data_path.mkdir(parents=True, exist_ok=True)
        
    def load_existing_data(self, jsonl_file_path):
        """
        ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ existing srf_sql_pairs.jsonl file load ‡¶ï‡¶∞‡ßá
        """
        try:
            data = []
            logger.info(f"Loading data from: {jsonl_file_path}")
            
            with open(jsonl_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():  # Empty line skip ‡¶ï‡¶∞‡¶ø
                        item = json.loads(line)
                        data.append(item)
            
            logger.info(f"Loaded {len(data)} SRF-SQL pairs")
            return data            
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            return []
    
    def clean_and_process_data(self, data):
        """
        Data clean ‡¶ï‡¶∞‡ßá processed format ‡¶è convert ‡¶ï‡¶∞‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶ø
        """
        processed_data = []
        
        for item in data:
            try:
                # Basic cleaning
                srf_text = self._clean_text(item.get('srf', ''))
                sql_query = self._clean_sql(item.get('sql', ''))
                supporting_table = item.get('supporting_table', '')
                
                # Skip empty data
                if not srf_text or not sql_query:
                    continue
                
                # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶ø
                metadata = self.extract_commission_metadata(srf_text)
                
                processed_item = {
                    'id': len(processed_data) + 1,
                    'srf_text': srf_text,
                    'sql_query': sql_query,
                    'supporting_table': supporting_table,
                    'srf_length': len(srf_text),
                    'sql_length': len(sql_query),
                    'commission_name': metadata['commission_name'],
                    'commission_type': metadata['commission_type']
                }
                
                processed_data.append(processed_item)
                
            except Exception as e:
                logger.warning(f"Error processing item: {str(e)}")
                continue
        
        # Categorize data
        processed_data = self.categorize_data_by_metadata(processed_data)
        
        logger.info(f"Processed {len(processed_data)} valid items")
        return processed_data
    
    def _clean_text(self, text):
        """Text clean ‡¶ï‡¶∞‡¶æ‡¶∞ simple function"""
        if not text:
            return ""
        
        # Basic cleaning
        text = str(text).strip()
        # Remove extra spaces
        text = ' '.join(text.split())
        # Remove special characters that might cause issues
        text = text.replace('\x00', '').replace('\r', '\n')
        
        return text
    
    def _clean_sql(self, sql):
        """SQL clean ‡¶ï‡¶∞‡¶æ‡¶∞ function"""
        if not sql:
            return ""
        
        sql = str(sql).strip()
        # Basic SQL formatting
        sql = sql.replace('\r\n', '\n').replace('\r', '\n')
        
        return sql
    
    def save_processed_data(self, processed_data, filename="processed_training_data.json"):
        """Processed data save ‡¶ï‡¶∞‡¶ø"""
        try:
            output_path = self.data_path / filename
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"Saved processed data to: {output_path}")
            return str(output_path)
            
        except Exception as e:
            logger.error(f"Error saving data: {str(e)}")
            return None
    
    def get_data_statistics(self, data):
        """Data ‡¶è‡¶∞ basic statistics"""
        if not data:
            return {}
        
        stats = {
            'total_items': len(data),
            'avg_srf_length': sum(item['srf_length'] for item in data) / len(data),
            'avg_sql_length': sum(item['sql_length'] for item in data) / len(data),
            'min_srf_length': min(item['srf_length'] for item in data),
            'max_srf_length': max(item['srf_length'] for item in data)
        }
        
        return stats
    
    def extract_commission_metadata(self, srf_text):
        """
        SRF ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶Æ‡¶ø‡¶∂‡¶® ‡¶ü‡¶æ‡¶á‡¶™ ‡¶è‡¶¨‡¶Ç ‡¶®‡¶æ‡¶Æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßá
        """
        commission_name = None
        commission_type = None
        
        try:
            # ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶ø
            report_title_pattern = r"Report Title:\s*(.+?)(?:\r|\n)"
            report_title_match = re.search(report_title_pattern, srf_text)
            
            if report_title_match:
                commission_name = report_title_match.group(1).strip()
                
                # ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ï‡¶Æ‡¶ø‡¶∂‡¶® ‡¶ü‡¶æ‡¶á‡¶™ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶ø
                commission_type = self._extract_commission_type(commission_name)
                
        except Exception as e:
            logger.warning(f"Error extracting metadata: {str(e)}")
        
        return {
            "commission_name": commission_name or "unknown",
            "commission_type": commission_type or "unknown"
        }
    
    def _extract_commission_type(self, commission_name):
        """
        ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶∞ ‡¶ï‡¶Æ‡¶ø‡¶∂‡¶® ‡¶®‡¶æ‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶Æ‡¶ø‡¶∂‡¶® ‡¶ü‡¶æ‡¶á‡¶™ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßá
        """
        if not commission_name:
            return "unknown"
            
        # ‡¶∏‡ßç‡¶ü‡ßá‡¶™ 1: ‡¶°‡ßá‡¶ü ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶ø
        date_patterns = [
            r'_\d+(?:st|nd|rd|th)?\s+(?:to|and)\s+\d+(?:st|nd|rd|th)?\s+[A-Za-z]+\'?\d+',  # _11th to 12th May22
            r'_\d+(?:st|nd|rd|th)?\s+[A-Za-z]+\'?\d+\s+to\s+\d+(?:st|nd|rd|th)?\s+[A-Za-z]+\'?\d+', # _1st Apr25 to 15th Apr25
            r'\d+(?:st|nd|rd|th)?\s+to\s+\d+(?:st|nd|rd|th)?\s+[A-Za-z]+\'?\d+', # 11th to 12th May22
            r'\d+(?:st|nd|rd|th)?\s+[A-Za-z]+\'?\d+', # 11th Apr22
            r'[A-Za-z]+\'?\d+', # May22, Apr'25
        ]
        
        commission_type = commission_name
        
        # ‡¶°‡ßá‡¶ü ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶ø
        for pattern in date_patterns:
            match = re.search(pattern, commission_type)
            if match:
                # ‡¶°‡ßá‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá‡¶∞ '_' ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡¶ø
                date_part = match.group(0)
                if date_part.startswith('_'):
                    date_part = date_part
                else:
                    date_part = f" {date_part}"
                commission_type = commission_type.replace(date_part, '')
                break
        
        # ‡¶∏‡ßç‡¶ü‡ßá‡¶™ 2: ‡¶∂‡ßá‡¶∑‡ßá‡¶∞ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ '_' ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡¶ø
        commission_type = commission_type.rstrip('_')
        
        # ‡¶∏‡ßç‡¶ü‡ßá‡¶™ 3: ‡¶∏‡ßç‡¶™‡ßá‡¶∂‡¶æ‡¶≤ ‡¶ï‡ßá‡¶∏ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤ ‡¶ï‡¶∞‡¶ø
        if commission_type.startswith("Commission for "):
            # "Commission for Nagad" ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶™‡ßÅ‡¶∞‡ßã ‡¶®‡¶æ‡¶Æ‡¶á ‡¶∞‡¶æ‡¶ñ‡¶ø
            return commission_type.strip()
        
        # ‡¶∏‡ßç‡¶ü‡ßá‡¶™ 4: MyBL Hourly, Portonics ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤ ‡¶ï‡¶∞‡¶ø
        if "MyBL" in commission_type and "Hourly" in commission_type:
            if "Portonics" in commission_type:
                return "MyBL Hourly Portonics"
            return "MyBL Hourly"
        
        if "BP Variable Incentive" in commission_type:
            return "BP Variable Incentive"
        
        # ‡¶∏‡ßç‡¶ü‡ßá‡¶™ 5: ‡¶§‡¶¨‡ßÅ‡¶ì ‡¶Ø‡¶¶‡¶ø '_' ‡¶•‡¶æ‡¶ï‡ßá ‡¶§‡¶¨‡ßá ‡¶∏‡¶¨‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∞‡¶æ‡¶ñ‡¶ø (RSO_RSO Supervisor_BP_GA ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá)
        if "_" in commission_type and any(x in commission_type for x in ["RSO", "BP", "GA"]):
            return commission_type.strip()
        
        return commission_type.strip()

    def categorize_data_by_metadata(self, processed_data):
        """
        Add categories to data for better filtering
        """
        for item in processed_data:
            srf_text = item['srf_text'].lower()
            
            # Categorize by commission type
            if 'hourly' in srf_text:
                item['sub_category'] = 'hourly'
            elif 'special day' in srf_text:
                item['sub_category'] = 'special_day'
            elif 'regular' in srf_text:
                item['sub_category'] = 'regular'
            else:
                item['sub_category'] = 'other'
                
            # Extract supporting tables info
            if item.get('supporting_table'):
                item['has_supporting_table'] = True
                item['table_info'] = self._extract_table_info(item['supporting_table'])
            else:
                item['has_supporting_table'] = False
                item['table_info'] = {}
        
        return processed_data
    
    def _extract_table_info(self, supporting_table):
        """Extract supporting tables and organize structured data"""
        table_info = {
            'columns': [],
            'row_count': 0,
            'has_deno': False,
            'has_commission': False
        }
        
        try:
            # Parse CSV data
            import io
            import pandas as pd
            csv_data = io.StringIO(supporting_table)
            df = pd.read_csv(csv_data)
            
            # Extract key columns
            table_info['columns'] = df.columns.tolist()
            table_info['row_count'] = len(df)
            
            # Check for important columns
            column_names = [col.upper() for col in df.columns]
            table_info['has_deno'] = any('DENO' in col for col in column_names)
            table_info['has_commission'] = any('COMMISSION' in col for col in column_names)
            
        except Exception as e:
            logger.warning(f"Could not parse supporting table: {e}")
            
        return table_info

# Usage example function
def process_your_data(jsonl_file_path):
    """
    ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ data process ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø main function
    """
    print("üöÄ Starting data processing...")
    
    # Data processor ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶ø
    processor = DataProcessor()
    
    # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ existing data load ‡¶ï‡¶∞‡¶ø
    raw_data = processor.load_existing_data(jsonl_file_path)
    
    if not raw_data:
        print("‚ùå No data found to process")
        return None
    
    # Data clean ‡¶ì process ‡¶ï‡¶∞‡¶ø
    processed_data = processor.clean_and_process_data(raw_data)
    
    # Statistics ‡¶¶‡ßá‡¶ñ‡¶ø
    stats = processor.get_data_statistics(processed_data)
    print("\nüìä Data Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # Processed data save ‡¶ï‡¶∞‡¶ø
    saved_path = processor.save_processed_data(processed_data)
    
    if saved_path:
        print(f"‚úÖ Data processing completed! Saved to: {saved_path}")
        return processed_data
    else:
        print("‚ùå Failed to save processed data")
        return None

# Direct run ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
if __name__ == "__main__":
    # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ existing data file path
    your_jsonl_file = "../MyBL_FEB_MAR_APR_MAY_25/srf_sql_pairs.jsonl"
    
    # Process ‡¶ï‡¶∞‡¶ø
    result = process_your_data(your_jsonl_file)
    
    if result:
        print(f"\nüéâ Successfully processed {len(result)} SRF-SQL pairs!")
    else:
        print("\nüòî Data processing failed!")
